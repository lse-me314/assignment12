---
title: "Assignment 12 - Word Embeddings"
author: "Jack Blumenau"
output: html_document
---

```{r, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
rm(list = ls())

library(tidyverse)
library(quanteda)
library(text2vec)

```

## Similarities, Analogies and Dictionary Expansion

Word embeddings are all the rage.[^seminar8-1] Whenever we have represented words as data in previous seminars, we have simply counted how often they occur across and within documents. We have largely viewed words as individual units: strings of text that uniquely identify a given meaning, for which we have had no natural notion of similarity for grouping similar words together.

[^seminar8-1]: A good qualitative indicator of the success of an innovation in quantitative methods is when they are discussed in some detail in the [London Review of Books](https://www.lrb.co.uk/the-paper/v45/n01/paul-taylor/on-chatgpt)

By contrast, word-embedding approaches represent each unique word in a corpus as dense, real-valued vector. As we discussed in the lecture, these vectors turn out to encode a lot of information about the ways in which words are used, and this information can put to good use across a wide range of questions.

In the seminar today, we will familiarise ourselves with some of the pre-trained word embeddings from the [GloVe project](https://nlp.stanford.edu/projects/glove/). We will use these vectors to discover similarities between words, to compute analogy-based tasks, and to supplement the dictionary-based approaches to measurement that we covered earlier in the course.

## Packages

You will need to load the following packages before beginning the assignment

```{r, echo = TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(quanteda)
library(text2vec)
# If you cannot load these libraries, try installing them first. E.g.: 
# install.packages("text2vec")
```

## Data

[**Download link for Glove embeddings**](https://dl.dropboxusercontent.com/s/vnsygzu3ymc6bwf/glove_embeddings.Rdata?dl=0)

Today we will be using the pre-trained GloVe embeddings, which can be downloaded from the link above. Note that the file which contains the word embeddings is very large! It may therefore take a minute or two to download, depending on your internet conection. Once you have downloaded the file, store it in the folder you have created for this assignment.

Despite the large size of the file, we are actually using one of the *smaller* versions of the GloVe embeddings, which were trained on a combination of Wikipedia and news data. The embeddings are of dimension 300 and cover some 400,000 words. Note that you could replicate any of the assignment here with larger versions of the GloVe embeddings by downloading them from the [GloVe project website](https://nlp.stanford.edu/projects/glove/), but any differences for the applications here are likely to be small.

## Word Similarities

1.  Load the glove embeddings into R using the `load()` function.

2.  Look at the dimensions of the `glove` embeddings object. How many rows does this object have? How many columns? What do these represent?

3.  Write a function to calculate the cosine similarity between a selected word and every other word in the glove embeddings object. I have provided some starter code below. You should be able to work out what goes in each part of this function by looking at the examples in the lecture slides. 

    You will need to use the `sim2()` function from the `text2vec` package here. Note that this function requires two main arguments: 1) `x` -- a matrix of embeddings. 2) `y` -- a second matrix of embeddings for which you would like to compute similarities. It is important to note that both of these inputs must be in matrix form. If you extract a vector from the glove object for a selected word, you have to transform it to a matrix for use with this function. To do so, use the `matrix` function, setting the `nrow` argument equal to 1.

    The function you create should take two inputs: 1) `target_word` -- the word for which you would like to calculate similarities;  2) `n` -- the number of nearest neighbouring words returned

```{r, eval = TRUE, echo = TRUE}

similarities <- function(target_word, n){

  # Extract embedding of target word by subsetting to the relevant row of the glove object
  
  # Calculate cosine similarity between target word and other words using the sim2 function
  
  # Report nearest neighbours of target word (i.e. those with the largest cosine similarity scores)

}

```


4.  Use your function to report the 7 most similar words to the words "quantitative", "text", and "analysis"?



## Word Analogies

1.  Write a function that computes analogies of the form "a is to b as c is to \_\_\_". For instance, if b is"king", a is "man", and c is "woman", then the missing word should be "queen".\
    
    Your function will need to take four arguments. Three arguments should correspond to the words included in the analogy. The fourth should be an argument specifying the number of nearest neighbouring words returned. Again, I have provided some starter code below, which you should be able to complete by consulting the lecture slides.


```{r, eval = TRUE, echo = TRUE}

analogies <- function(a, b, c, n){
  
  # Extract vectors for each of the three words in analogy task by subsetting the glove matrix
  
  # Generate analogy vector: vector(c) - vector(a) + vector(b)
  
  # Calculate cosine similarity between anaology vector and all other vectors using the sim2 function
  
  # Report nearest neighbours of analogy vector

}

```


2.  Use the function you created above to find the word-embedding answers to the following analogy completion tasks.

    -   Einstein is to scientist as Picasso is to \_\_\_?
    -   Arsenal is to football as Yankees is to \_\_\_?
    -   Actor is to theatre as doctor is to \_\_\_?

3.  Come up with some of your own analogies and try them here.

## Dictionary Expansion

In this exercise, we will used the Moral Foundations Dictionary to score some Reddit posts in terms of their moral content. 

We will use two sources of data for this part of the assignment:

```{r, echo = TRUE, eval = TRUE, warning=FALSE, message=FALSE}

mft_dictionary_words <- read_csv("mft_dictionary.csv")
mft_texts <- read_csv("mft_texts.csv")

```

- **Moral Foundations Dictionary** -- `mft_dictionary.csv`

    -   This file contains lists of words that are thought to indicate the presence of different moral concerns in text. The dictionary was originally developed by Jesse Graham and Jonathan Haidt and is described in more detail [in this paper](https://psycnet.apa.org/doiLanding?doi=10.1037%2Fa0015141).
    -   The file includes 5 categories of moral concern -- authority, loyalty, santity, fairness, and care -- each of which is associated with a different list of words

- **Moral Foundations Reddit Corpus** -- `mft_texts.csv`

    -   This file contains `r nrow(mft_texts)` English Reddit comments that have been curated from 11 distinct subreddits. In addition to the texts -- which cover a wide variety of different topics -- the data also includes hand-annotations by trained annotators for the different categories of moral concern described by Moral Foundations Theory.


1.  Create a vector of the MFT "Care" words.

2.  Extract the embeddings from the `glove` object relating to the care words.

3.  Calculate the mean embedding vector of the care words. To do this, use the `colMeans` function, which will calculate the mean of each column of the matrix.

4.  Calculate the similarity between the mean care vector and every other word in the `glove` embedding object. To do so, use the `sim2()` function again.

5.  What are the 500 words that have highest cosine similarity with the mean care vector? How many of these words are in the original dictionary?

6.  Examine the words that are in the top 500 words that you calculated above but which are *not* in the original care dictionary. Do these represent the concept of care?

7.  What does your answer to the previous question suggest about this dictionary expansion approach?

## Classification Accuracy

The `mft_texts` object includes a series of variables that record the human annotations of which category each text falls into. In this part of the assignment, you will use dictionary-based methods to score the texts, and compare the dictionary scores to those human codings. If you have forgotten how to apply dictionaries, go back and look at the material from day 9.

1.  Create a new dictionary which includes two categories. The first should be a `care_original_words` category, which contains only the words from the original care dictionary. The second should be a `care_embedding_words` which contains both the original care words and the top 500 words that you extracted in the last section.

2.  Use the dictionary you just constructed to score the scores in the `mft_texts` object. Create variables in that object that indicate whether a given dictionary classifies each text as a care text or not (i.e. classify a text as a care text if it contains any words from the relevant dictionary).

3.  Create a confusion matrix which compares the human annotations to the scores generated by the dictionary analysis. Which performs best, the original dictionary or the word-embedding approach?

