---
title: "Assignment 11 - Topic Models (Solutions)"
author: "Jack Blumenau"
output: html_document
---

You will need to load the following libraries (you may also want to set the random number seed to make everything replicable):
```{r, eval=T, message = F}
library(quanteda)
library(topicmodels)
library(LDAvis)
library(stm)
library(knitr)
library(lda)
library(servr)
set.seed(221186)
```

## Topic modelling of parliamentary speeches

In this question we are going to use topic modelling to understand how parliamentary speech varies by the gender of the MP. We will be working with a corpus of speeches made by legislators in the UK House of Commons in the 2014 calandar year. 

You will need to make sure that the file `hoc_speeches.Rdata` is in your current working directory, and then use the following command to read this data into `R`.

```{r, eval = FALSE, message = FALSE, echo = TRUE}
load("hoc_speeches.Rdata")
```

```{r, message = FALSE, eval = TRUE, echo = FALSE}
load("data/hoc_speeches.Rdata")
```
 
(a) Inspect the `data.frame` object `speeches` and produce some summary statistics.

```{r}
prop.table(table(speeches$party, speeches$gender),1)

speeches$ntoken <- ntoken(speeches$speech)
hist(speeches$ntoken, main = "Distribution of speech length", breaks = 100)
```

(b) Use the functions in the `quanteda` package to turn this data into a `corpus` object.

```{r}

speechCorpus <- corpus(speeches, text_field = "speech")

```

(c) Turn this corpus into a tokens object and then into a document-feature matrix. At a minimum, you should remove punctuation and numbers from the texts when constructing the `tokens` object (`remove_punct =T` & `remove_numbers = T`) but you may also want to do some additional pre-processing if you don't want to wait days for your topic model to coverge. Think about some of the following:
 
    (i) Unigrams? 
    (ii) Stopwords?
    (iii) Stemming?
    (iv) Very infrequent words?

```{r, message = FALSE}
speechDFM <- tokens(speechCorpus, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_wordstem(language = "en") %>%
  tokens_remove(stopwords("en")) %>%
  dfm()

speechDFM <- dfm_trim(speechDFM, min_termfreq = 5, min_docfreq = 0.0025, docfreq_type = "prop")
```

(d) Run a structural topic model for this corpus, using the `gender` variable in the topic prevalence argument. Use the `stm` function to do this. Set the `seed` argument to `stm` to be equal to `123`. Be aware, this takes about 3 minutes to run on Jack's iMac -- for testing purposes you might want to set the maximum iterations for the stm to be some low number (`max.em.its = 10` for instance). Remember, you will need to select a value for `K` (I have used `K = 20`).

  Specify and estimate the `stm` model:

```{r, cache = T, message=FALSE}

K <- 20
stmOut <- stm(documents = speechDFM, 
              data = docvars(speechDFM),
              prevalence = ~ gender,
              K = K, seed = 123, verbose = FALSE, max.em.its = 500)

```

  Plot the estimated topic model using the `plot()` function on your estimated topic model:

```{r}
plot(stmOut)
```

(e) Examine the top words from each topic

```{r}
topic_labels <- labelTopics(stmOut)
topic_labels <- apply(topic_labels$prob,1, function(x) paste(x, collapse=";"))
print(topic_labels)
```


(f) Find the top three documents associated with each topic. Do these make sense given the words you have used to describe that topic? In the estimated `stm` object, the document-topic probabilities are stored in `theta`. You will need to extract these probabilities, and then, for each topic, find the documents with the highest scores (the `order()` function might be helpful here). Report the top speeches for one selected topic.
  
```{r}

top_docs <- apply(stmOut$theta, 2, function(x) order(x, decreasing = TRUE)[1:3])

top_school_docs <- top_docs[,grep("school", topic_labels)]

as.character(speechCorpus)[top_school_docs[1]]

```

(g) Use the `estimateEffect` and `plot.estimateEffect` functions in the `stm` package to estimate the effect of MP gender on topic usage. On which topics are women, on average, more active? 

  Hint: `estimateEffect` takes three main arguments. 

  1) `formula`, which specifies the model for the topic proportions. You do not need to specify the dependent variable here (i.e. the topic proportions) as R will infer that automatically, so you just need a formula of the form `~ independent_variable`
  2) `stmobj`, which is just the estimated model output from the `stm()` function
  3) `metadata`, which is the data.frame that contains the relevant covariates (try `docvars(my_dfm)`)
  
  `plot.estimateEffect` takes a large number of options which affect the presentation of the results. Look at `?plot.estimateEffect` for more details.

```{r}

est_gender_effect <- estimateEffect(~gender, stmOut, metadata = docvars(speechDFM))

plot.estimateEffect(est_gender_effect, "gender", method = "difference", 
                    cov.value1 = "female", cov.value2 = "male", 
                    labeltype = "frex", n = 3, verbose.labels = FALSE,
                    model = stmOut)

```

**Women appear to speak somewhat more frequently about the `nhs;health;patient` topic, though the significance of any of this effect in this data is questionable.**

## Topic modelling of movie reviews

2.  **movies corpus**.  Here we will use the very impressive `LDAvis` library in conjunction with the `lda::lda.collapsed.gibbs.sampler()` function from the `lda` package. The following code is used to demonstate how the parliamentary speeches interactive visualisation example was created for in the lecture. Your task is to implement this for the `movies` corpus.

First we construct the relevant `dfm` and estimate the `lda` model.
```{r, eval=FALSE}

## Create a corpus of speeches
speechCorpus <- corpus(speeches, text_field = "speech")

## Convert to a tokens object
speechTokens <- tokens(speechCorpus, remove_punct = TRUE, remove_numbers = TRUE)

## Convert to dfm, removing some words that appear very regularly
speechDfm <- dfm(speechTokens) %>%
  dfm_remove(pattern = c(stopwords("en"), "will", "hon", "right", "people", "government", 
                  "can", "friend", "house", "gentleman", "said", "interruption", 
                  "prime", "minister", "secretary", "state"))

## Trim some rarely occuring words
speechDfm <- dfm_trim(speechDfm, min_termfreq = 15, 
                      min_docfreq = 0.0015, docfreq_type = "prop")

# Convert to lda format
speechDfmlda <- convert(speechDfm, to = "lda")

# MCMC and model tuning parameters:
K <- 30 # Number of topics
G <- 2000 # Number of iterations
alpha <- 0.02 # Prior for topic proportions
eta <- 0.02 # Prior for topic distributions

# Fit the model
t1 <- Sys.time() # Start timer

fit <- lda.collapsed.gibbs.sampler(documents = speechDfmlda$documents, K = K, 
                                   vocab = speechDfmlda$vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time() # End timer
t2 - t1  # about 7 minutes on Jack's iMac
```

Now we plot the model using `LDAvis`.

```{r, eval=FALSE}
library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x))), 
                   theta = t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x))), 
                   doc.length = ntoken(speechDfm), 
                   vocab = featnames(speechDfm), 
                   term.frequency = colSums(speechDfm))
        
serVis(json, out.dir = "exampleVis", open.browser = TRUE)
```

(a) You will need to load the data from the `quantedaData` package which is hosted on GitHub: 
    
```{r}
#remotes::install_github("kbenoit/quantedaData")
data(data_corpus_movies, package = "quantedaData")
```
    
(b) Adapt the code above to produce an interactive visualisation of the `movies` corpus. 
    
```{r, cache = TRUE, eval = TRUE}

# prepare the texts
moviesDfm <- tokens(data_corpus_movies, remove_punct = TRUE, remove_numbers = TRUE) %>%
  dfm() %>%
  dfm_remove(pattern = stopwords("en")) %>%
  dfm_trim(min_termfreq = 5)

# MCMC and model tuning parameters:
K <- 20
G <- 2000
alpha <- 0.02
eta <- 0.02

# convert to lda format
moviesDfmlda <- convert(moviesDfm, to = "lda")

# fit the model
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = moviesDfmlda$documents, K = K, 
                                   vocab = moviesDfmlda$vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 5 minutes on Jack's iMac

library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x))), 
                   theta = t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x))), 
                   doc.length = ntoken(moviesDfm), 
                   vocab = featnames(moviesDfm), 
                   term.frequency = colSums(moviesDfm))

#serVis(json, out.dir = "visColl", open.browser = TRUE)
```
    
(c) Describe a few topics as you see them.  Is there a "scary movie" topic?  Is there a "science fiction" topic?  

(d) Use the estimated document probabilities to extract the text of one movie from the topic that most appeals to you. Would you want to watch that movie? Your answer will clearly reveal something about your personality.

    The output of `lda.collapsed.gibbs.sampler()` doesn't actually include the topic proportions for each document. Instead, the `fit$document_sums` element includes the number of times that each word in each document (columns) were assigned to each topic (rows). To translate these into proportions, we need to divide the word count for each document by the total number of words in the document. Rather than doing this row by row, try using the `apply()` function to apply the same calculation to every column in the matrix. See `?apply()` for more help on this.
    
    To get the top scoring words for each topic, use the `top.topic.words()` function (see the help file for more information).

```{r}

# Covert document-topic word counts to document-topic proportions
document_proportions <- t(apply(fit$document_sums, 2, function(x) x/sum(x)))

# Extract top topic words
top.topic.words(fit$topics,5,by.score = TRUE)
# Clearly topic 2, the "disney;story;animated;animation;tarzan" topic, is the most appealing here

# Extract top document from that topic
top_disney_film <- which.max(document_proportions[,2])

as.character(data_corpus_movies[top_disney_film])

# Mulan is a great film!

```
    
    